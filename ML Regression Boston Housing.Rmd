---
title: 'Regression Case Study: Boston Housing'
author: "Michael Harrison"
date: "May 1, 2017"
output: html_document
---


1. CRIM: per capita crime rate by town
2. ZN: proportion of residential land zoned for lots over 25,000 sq.ft.
3. INDUS: proportion of non-retail business acres per town
4. CHAS: Charles River dummy variable (= 1 if tract bounds river; 0 otherwise) 
5. NOX: nitric oxides concentration (parts per 10 million)
6. RM: average number of rooms per dwelling
7. AGE: proportion of owner-occupied units built prior to 1940
8. DIS: weighted distances to five Boston employment centers
9. RAD: index of accessibility to radial highways
10. TAX: full-value property-tax rate per $10,000
11. PTRATIO: pupil-teacher ratio by town
12. B: 1000(Bk âˆ’ 0.63)2 where Bk is the proportion of blacks by town 
13. LSTAT: % lower status of the population
14. MEDV: Median value of owner-occupied homes in $1000s

# 1. Prepare Problem
# a) Load packages
```{r}
library(mlbench)
library(caret)
library(corrplot)
```

# b) Load dataset
```{r}
data("BostonHousing")
```

# c) Split-out validation dataset
```{r}
inTrain <- createDataPartition(BostonHousing$medv, p=0.8, list=FALSE)
training <- BostonHousing[inTrain,]
testing <- BostonHousing[-inTrain,]
```

# 2. Summarize Data
# a) Descriptive statistics
```{r}
dim(training)
```
```{r}
sapply(training, class)
```

```{r}
head(training, 6)
```
```{r}
summary(training)
```

```{r}
training[,4] <- as.numeric(as.character(training[,4]))
cor(training[,1:13])
```

# b) Data visualizations

```{r}
par(mfrow = c(3,5))
for(i in 1:13){
        hist(training[,i], main = names(training)[i], xlab = names(training)[i])
}

```

```{r}
par(mfrow = c(3,5))
for(i in 1:13){
        plot(density(training[,i]), 
             main = names(training)[i], xlab = names(training)[i])
}
```

```{r}
par(mfrow = c(3,5))
for(i in 1:13){
        boxplot(training[,i], main = names(training)[i], xlab = names(training)[i])
}
```

```{r}
pairs(training[,1:14])
```

```{r}
correlations <- cor(training[,1:13])
corrplot(correlations, method = "circle")
```

# 4. Evaluate Algorithms
# a) Test options and evaluation metric
```{r}
fitControl <- trainControl(method = "repeatedcv", number = 10, repeats = 3)
metric <- "RMSE"
seed <- set.seed(9)
```

# c) Compare Algorithms
```{r}
#Linear Model
set.seed(seed)
fitLM <- train(medv~., data = training, 
               method = "lm", metric = metric, 
               preProc = c("center", "scale"), trControl = fitControl)
#Generalized Linear Model
set.seed(seed)
fitGLM <- train(medv~., data = training, 
                method = "glm", metric = metric, 
                preProc = c("center", "scale"), trControl = fitControl)
#GLM.net
set.seed(seed)
fitGLMnet <- train(medv~., data = training,
                   method = "glmnet", metric = metric,
                   preProc = c("center", "scale"), trControl = fitControl)
#Support Vector Machines
set.seed(seed)
fitSVM <- train(medv~., data = training,
                method = "svmRadial", metric = metric,
                preProc = c("center", "scale"), trControl = fitControl)
#Classification And Regression Trees
set.seed(seed)
grid <- expand.grid(.cp = c(0,0.05,0.1))
fitCART <- train(medv~., data = training,
                 method = "rpart", metric = metric, tuneGrid = grid,
                 preProc = c("center", "scale"), trControl = fitControl)
#K-Nearest Neighbors
set.seed(seed)
fitKNN <- train(medv~., data = training,
                method = "knn", metric = metric,
                preProc = c("center", "scale"), trControl = fitControl)
 
results <- resamples(list(LM = fitLM, 
                          GLM = fitGLM,
                          GLMNET = fitGLMnet,
                          SVM = fitSVM,
                          CART = fitCART,
                          KNN = fitKNN))
summary(results)
```

```{r}
scales <- list(x = list(relation = "free"), y = list(relation = "free"))
dotplot(results, scales = scales)
```

- Feature Selection
```{r}
set.seed(seed)
cutoff <- 0.70
correlations <- cor(training[,1:13])
highCorrelation <- findCorrelation(correlations, cutoff = cutoff)
for(value in highCorrelation){
        print(names(training[value]))
}
```
```{r}
trainingFeatures <- training[-highCorrelation]
dim(trainingFeatures)
```
- Re-run Algorithms
```{r}
#Linear Model
set.seed(seed)
fitLM <- train(medv~., data = trainingFeatures, 
               method = "lm", metric = metric, 
               preProc = c("center", "scale"), trControl = fitControl)
#Generalized Linear Model
set.seed(seed)
fitGLM <- train(medv~., data = trainingFeatures, 
                method = "glm", metric = metric, 
                preProc = c("center", "scale"), trControl = fitControl)
#GLM.net
set.seed(seed)
fitGLMnet <- train(medv~., data = trainingFeatures,
                   method = "glmnet", metric = metric,
                   preProc = c("center", "scale"), trControl = fitControl)
#Support Vector Machines
set.seed(seed)
fitSVM <- train(medv~., data = trainingFeatures,
                method = "svmRadial", metric = metric,
                preProc = c("center", "scale"), trControl = fitControl)
#Classification And Regression Trees
set.seed(seed)
grid <- expand.grid(.cp = c(0,0.05,0.1))
fitCART <- train(medv~., data = trainingFeatures,
                 method = "rpart", metric = metric, tuneGrid = grid,
                 preProc = c("center", "scale"), trControl = fitControl)
#K-Nearest Neighbors
set.seed(seed)
fitKNN <- train(medv~., data = trainingFeatures,
                method = "knn", metric = metric,
                preProc = c("center", "scale"), trControl = fitControl)
 
featureResults <- resamples(list(LM = fitLM, 
                          GLM = fitGLM,
                          GLMNET = fitGLMnet,
                          SVM = fitSVM,
                          CART = fitCART,
                          KNN = fitKNN))
summary(featureResults)
```
```{r}
dotplot(featureResults, scales = scales)
```
- Applying Box Cox transformation
```{r}
#Linear Model
set.seed(seed)
fitLM <- train(medv~., data = training, 
               method = "lm", metric = metric, 
               preProc = c("center", "scale", "BoxCox"), trControl = fitControl)
#Generalized Linear Model
set.seed(seed)
fitGLM <- train(medv~., data = training, 
                method = "glm", metric = metric, 
                preProc = c("center", "scale", "BoxCox"), trControl = fitControl)
#GLM.net
set.seed(seed)
fitGLMnet <- train(medv~., data = training,
                   method = "glmnet", metric = metric,
                   preProc = c("center", "scale", "BoxCox"), trControl = fitControl)
#Support Vector Machines
set.seed(seed)
fitSVM <- train(medv~., data = training,
                method = "svmRadial", metric = metric,
                preProc = c("center", "scale", "BoxCox"), trControl = fitControl)
#Classification And Regression Trees
set.seed(seed)
grid <- expand.grid(.cp = c(0,0.05,0.1))
fitCART <- train(medv~., data = training,
                 method = "rpart", metric = metric, tuneGrid = grid,
                 preProc = c("center", "scale", "BoxCox"), trControl = fitControl)
#K-Nearest Neighbors
set.seed(seed)
fitKNN <- train(medv~., data = training,
                method = "knn", metric = metric,
                preProc = c("center", "scale", "BoxCox"), trControl = fitControl)
 
transformResults <- resamples(list(LM = fitLM, 
                          GLM = fitGLM,
                          GLMNET = fitGLMnet,
                          SVM = fitSVM,
                          CART = fitCART,
                          KNN = fitKNN))
summary(transformResults)
```
```{r}
dotplot(transformResults, scales = scales)
```



# 5. Improve Accuracy
# a) Algorithm Tuning
```{r}
fitSVM
```
- Tuning the Support Vector Machine Algorithm
```{r}
fitControl <- trainControl(method = "repeatedcv", number = 10, repeats = 3)
metric <- "RMSE"
set.seed(seed)
grid <- expand.grid(.sigma = c(0.025, 0.05, 0.075, 0.1, 0.125, 0.15),
                    .C = seq(1, 10, by=1))
fitSVM <- train(medv~., data=training,
                method = "svmRadial", metric = metric, tuneGrid = grid,
                preProc = c("BoxCox"), trControl = fitControl)
fitSVM
```
```{r}
plot(fitSVM)
```

# b) Ensembles
```{r}
fitControl <- trainControl(method = "repeatedcv", number =10, repeats = 3)
metric = "RMSE"
#Random Forest
set.seed(seed)
fitRF <- train(medv~., data = training, 
               method = "rf", metric = metric,
               preProc = c("BoxCox"), trControl = fitControl)
#Stochastic Gradient Boosting
set.seed(seed)
fitGBM <- train(medv~., data = training,
                method = "gbm", metric = metric, 
                preProc = c("BoxCox"), trControl = fitControl, verbose = FALSE)
#Cubist
set.seed(seed)
fitCUBIST <- train(medv~., data = training, 
                   method = "cubist", metric = metric, 
                   preProc = c("BoxCox"), trControl = fitControl)

ensembleResults <- resamples(list(RF = fitRF, GBM = fitGBM, CUBIST = fitCUBIST))
summary(ensembleResults)
```

```{r}
dotplot(ensembleResults, scales = scales)
```

```{r}
fitCUBIST
```


- Tuning Cubist Algorithm
```{r}

```

# 6. Finalize Model
# a) Predictions on validation dataset
# b) Create standalone model on entire training dataset
# c) Save model for later use